{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54673188-f2b5-4a3f-9194-b2766a66592b",
   "metadata": {},
   "source": [
    "### Excercise 1.1: Self-Play\n",
    "\n",
    "Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself. What do you think would happen in this case? Would it learn a different way of playing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a58813-346b-469c-ab9c-34dbec5c17d9",
   "metadata": {},
   "source": [
    "The self-play and random opponent will likely learn differently in the beginning, but will eventually converge to the same policy. When training with self play the opponent will revisit board states in which it found success and avoid board states in which it lost. The board states explored are likely to be different from random. Self-play will also explore more unique board stats faster because the RL agent would become unlikely to lose in the first 3 moves, but the random agent is more likely than not to lose in the first 3 moves.\n",
    "\n",
    "In the long run, however, the RL agent being trained with self-play and a random opponent will converge to the same policy. The agents in the self-play game will eventually never make mistakes on \"exploit\" moves, they would only make mistakes on random \"explore\" moves. Given that the exploit moves always return 0 reward for a draw against the opposing agents exploit moves, it would only learn from taking advantage or the random explore moves. In this way, it will learn to play the same as the agent playing with random moves.\n",
    "\n",
    "In both cases the number of games needed to converge to this policy is very large. Against the random opponent, every possible board state would need to be explored multiple times through random chance. Against the RL opponent, both opponents need to fully explore the state space multiple times to create RL agents that don't make mistakes and then re-explore the entire state space via random explore move multiple times. \n",
    "\n",
    "Note that in the self-play case the agent does not learn twice as quickly per game because the same agent is playing on both sides. every O move board state is only possible as an O move and every X move is only possible as an X move. X always moves with an even number of markers on the board while O always moves with an odd number. training the same agent on both sides is equivalent to training two different RL agents on each side of the board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de505782-62a2-4ed6-a7c8-09d8aad861b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Excercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60733b08-830c-474b-9cd4-308eada2473b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
